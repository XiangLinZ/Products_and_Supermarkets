{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from datetime import date\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import src.herramientas_scrapeo as hs\n",
    "import src.biblioteca as bb\n",
    "import src.soporte as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring options of Selenium\n",
    "opciones= Options()\n",
    "opciones.add_experimental_option('excludeSwitches', ['enable-automation']) # To hide as bot\n",
    "opciones.add_experimental_option('useAutomationExtension', False)\n",
    "opciones.add_argument('--start-maximized') # Starting with the windows maximized\n",
    "opciones.add_argument('user.data-dir=selenium') # Saving cookies\n",
    "opciones.add_argument('--incognito') # incognit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring some variables for later usage\n",
    "productos_mercadona = bb.productos_mercadona_estructura.copy() # Structure of the future dataframe\n",
    "recorrer = bb.listacategorias_merc.copy() # a list of categories that I want to scrap\n",
    "errores = set() # A set where i store the categories that coundn't been scraped well\n",
    "today = date.today() # Making a variable with actual Date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the cell below I make some ajust to scrap better the data that i could scrap, this happens because Mercadona has a anti-bot system.... ðŸ˜¥ðŸ˜¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfincompleto = pd.read_csv(\"../data/merc2023-01-20_suma.csv\") # Loading the data that I already scraped\n",
    "for cat_id in dfincompleto[\"category_id\"].unique(): # Searching the categories that was already scraped\n",
    "    if cat_id in recorrer:\n",
    "        recorrer.remove(cat_id) # And droping them from the list of categories that i want to scrap\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping with a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.scrap_mercadona(recorrer, productos_mercadona, errores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making it a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With our data with structure of a diccionary I make a dataframe\n",
    "dfm = pd.DataFrame(productos_mercadona)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy\n",
    "dfm2 = dfm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of items that I've scraped\n",
    "dfmcate_id = dfm2[\"category_id\"].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is some that didn't scrap well so i search them\n",
    "dfmcate_id.apply(lambda x: sp.revisar_errores(x[\"index\"], x[\"category_id\"], errores), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And after drop them  to scrap then properly later\n",
    "for error in errores:\n",
    "    dfm2 = dfm2[dfm2[\"category_id\"] != error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping some products that doesn't have all the info that I'm searching for\n",
    "dfm2.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing our data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it's the second try scraping, first, we need to load our CSV where had storaged our data and merge it with the data that we just scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfsuma = pd.read_csv(\"../data/merc2023-01-20_suma.csv\")\n",
    "dfsuma2 = pd.concat([dfsuma, dfm2], axis = 0, join = \"outer\", ignore_index = True)\n",
    "dfsuma2.to_csv(f\"../data/merc{today}_suma.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it's the first time scraping, we storage our data into a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm2.to_csv(f\"../data/merc{today}_suma.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0fe1027508b216a896019249be6f800db5587dd8e29811ed6b75b25cd3ed733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
